{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99750af0",
   "metadata": {},
   "source": [
    "# Retrieval Evaluation Notebook \n",
    "\n",
    "This notebook contains controlled experiments to evaluate the **retrieval component** of a RAG system using an **LLM-as-a-judge**. Two parameters are compared:\n",
    "\n",
    "1. **Chunking strategy**\n",
    "\n",
    "   * Markdown header-based chunking\n",
    "   * Fixed-size chunking\n",
    "2. **Retrieval search type**\n",
    "\n",
    "   * Similarity\n",
    "   * MMR (Max Marginal Relevance)\n",
    "\n",
    "The outcome is a comparative ranking of retrieval sets, scored across quality dimensions, with a final winner per query.\n",
    "\n",
    "---\n",
    "\n",
    "## Data\n",
    "\n",
    "**Source:** Academic project reports and technical documentation\n",
    "**Formats:** PDF and Markdown (`.pdf`, `.md`)\n",
    "\n",
    "---\n",
    "\n",
    "## Tooling\n",
    "\n",
    "* **Vector DB:** Qdrant (running via Docker)\n",
    "* **Embedding model:** `text-embedding-3-small`\n",
    "* **Reranker:** `cross-encoder/ms-marco-MiniLM-L-6-v2`\n",
    "* **PDF → Markdown conversion:** Docling\n",
    "* **OCR:** EasyOCR\n",
    "* **Retrieval framework:** LangChain\n",
    "* **Judge LLM:** OpenAI API (`o3-mini`)\n",
    "* **Structured evaluation outputs:** Pydantic\n",
    "\n",
    "---\n",
    "\n",
    "## Experiment Design\n",
    "\n",
    "Each query is executed under **4 retrieval conditions**:\n",
    "\n",
    "| Set | Chunking Strategy   | Retrieval Type |\n",
    "| --- | ------------------- | -------------- |\n",
    "| A   | Markdown chunking   | Similarity     |\n",
    "| B   | Markdown chunking   | MMR            |\n",
    "| C   | Fixed-size chunking | Similarity     |\n",
    "| D   | Fixed-size chunking | MMR            |\n",
    "\n",
    "All retrieved documents are **reranked** with a cross-encoder before being passed to the judge.\n",
    "\n",
    "---\n",
    "\n",
    "## Chunking Strategies\n",
    "\n",
    "### Strategy 1: Markdown Chunking (Structure-aware)\n",
    "\n",
    "**Markdown files**\n",
    "\n",
    "* Split using `MarkdownHeaderTextSplitter`\n",
    "* Header levels: `#`, `##`, `###`, `####`\n",
    "\n",
    "**PDF files**\n",
    "\n",
    "* Convert PDF → Markdown using **Docling + EasyOCR**\n",
    "* Then split using `MarkdownHeaderTextSplitter`\n",
    "\n",
    "This strategy preserves document hierarchy and aligns chunks with semantic structure.\n",
    "\n",
    "---\n",
    "\n",
    "### Strategy 2: Fixed-Size Chunking (Uniform + robust)\n",
    "\n",
    "**Markdown files**\n",
    "\n",
    "1. Split into sections by headers\n",
    "2. Chunk using `RecursiveCharacterTextSplitter`\n",
    "\n",
    "   * `chunk_size = 1172`\n",
    "   * `overlap = 0`\n",
    "\n",
    "**PDF files**\n",
    "\n",
    "1. Split by page\n",
    "2. Chunk using `RecursiveCharacterTextSplitter`\n",
    "\n",
    "   * `chunk_size = 2090`\n",
    "   * `overlap = 200`\n",
    "\n",
    "This strategy aims for consistent chunk lengths and improved recall under dense text.\n",
    "\n",
    "---\n",
    "\n",
    "## Rationale for Chunk Sizes\n",
    "\n",
    "Chunk sizes were selected empirically by analyzing **character-count distributions** of PDF pages and Markdown sections using **histograms and boxplots**.\n",
    "\n",
    "### PDF files\n",
    "\n",
    "* Page lengths showed a broad but stable distribution (no extreme outliers)\n",
    "* Pages generally contained more text per unit than Markdown sections\n",
    "  → **Chunk size chosen:** **median page character count = 2090**\n",
    "\n",
    "### Markdown files\n",
    "\n",
    "* Section lengths were skewed with large outliers\n",
    "* Typical sections were much shorter than the extremes\n",
    "  → **Chunk size chosen:** **maximum non-outlier value = upper whisker = Q3 + 1.5×IQR = 1172**\n",
    "\n",
    "---\n",
    "\n",
    "## Retrieval Search Types\n",
    "\n",
    "1. **Similarity** (dense vector similarity search)\n",
    "2. **MMR** (diversity-aware retrieval, trades off relevance vs novelty)\n",
    "\n",
    "---\n",
    "\n",
    "## LLM-as-a-Judge Setup\n",
    "\n",
    "### Queries\n",
    "\n",
    "A list of **10 evaluation queries** was prepared, with an approximate expectation of what chunks should be retrieved for each.\n",
    "\n",
    "### Judge model choice\n",
    "\n",
    "A **cost-effective, small reasoning model with high intelligence** (`o3-mini`) was used because:\n",
    "\n",
    "* The task is comparative ranking (selection + scoring),\n",
    "* It requires less reasoning than generating a full final answer.\n",
    "\n",
    "### Prompt iterations (what changed)\n",
    "\n",
    "1. **Baseline prompt**\n",
    "\n",
    "   * Too vague / open-ended\n",
    "   * Produced unsatisfying scoring behavior\n",
    "2. **Improved prompt**\n",
    "\n",
    "   * More explicit role definition\n",
    "   * Clear scoring criteria + 0–5 scale\n",
    "   * Decision rules (winner selection, insufficient handling)\n",
    "   * Forced **structured output** (Pydantic schema)\n",
    "\n",
    "### Stability issue and fix\n",
    "\n",
    "* Even with the improved prompt, results were **inconsistent across 3 runs**\n",
    "* Error analysis identified a key issue:\n",
    "\n",
    "  * **Markdown splitter was stripping headings**, weakening chunk semantics and harming judge reliability\n",
    "* Fix:\n",
    "\n",
    "  * Updated Markdown splitter configuration to **preserve headings** in `page_content`\n",
    "\n",
    "After the fix, across the next 3 runs:\n",
    "\n",
    "* The judge produced a **consistent winner 8/10 times**\n",
    "\n",
    "---\n",
    "\n",
    "## Result Summary\n",
    "\n",
    "**Winner:** **Markdown chunking strategy + Similarity retrieval**\n",
    "\n",
    "This combination produced the strongest balance of:\n",
    "\n",
    "* semantic relevance (header-aligned chunks),\n",
    "* sufficient coverage for query intent,\n",
    "* lower noise compared to fixed chunking under these documents.\n",
    "\n",
    "---\n",
    "\n",
    "## Notes / Takeaways\n",
    "\n",
    "* Evaluation stability depends heavily on **chunk content fidelity** (e.g., preserving headings).\n",
    "* LLM judges can be sensitive to formatting changes; enforcing structure via Pydantic helps, but **instruction quality dominates**.\n",
    "* For technical academic documents, **structure-aware chunking** paired with straightforward similarity retrieval was most reliable in this setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b42ffcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from json import load, dump\n",
    "from dataclasses import dataclass\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from re import MULTILINE, DOTALL, compile\n",
    "from typing import Any, List, Literal, Sequence, Tuple, Annotated\n",
    "\n",
    "from docling.datamodel.accelerator_options import (\n",
    "    AcceleratorDevice,\n",
    "    AcceleratorOptions,\n",
    ")\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    EasyOcrOptions,\n",
    "    PdfPipelineOptions,\n",
    ")\n",
    "from docling.document_converter import (\n",
    "    DocumentConverter,\n",
    "    PdfFormatOption,\n",
    ")\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_text_splitters import (\n",
    "    MarkdownHeaderTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "from openai import OpenAI\n",
    "\n",
    "from pymupdf import Document as PdfDocument\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdddb222",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStoreBase:\n",
    "    COLLECTION: str\n",
    "    PROCESSOR_CLS: type\n",
    "\n",
    "    def __init__(self, url=\"http://localhost:6333\", port=6333):\n",
    "        self.url = url\n",
    "        self.port = port\n",
    "\n",
    "    def get_vector_store(self):\n",
    "        client = QdrantClient(url=self.url, port=self.port)\n",
    "\n",
    "        if client.collection_exists(self.COLLECTION):\n",
    "            return self._load()\n",
    "\n",
    "        return self._create()\n",
    "\n",
    "    def _create(self):\n",
    "        processor = self.PROCESSOR_CLS()\n",
    "        documents = processor.build_documents()\n",
    "\n",
    "        return QdrantVectorStore.from_documents(\n",
    "            documents=documents,\n",
    "            embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    "            collection_name=self.COLLECTION,\n",
    "            url=self.url,\n",
    "            port=self.port,\n",
    "        )\n",
    "\n",
    "    def _load(self):\n",
    "        return QdrantVectorStore.from_existing_collection(\n",
    "            embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"),\n",
    "            collection_name=self.COLLECTION,\n",
    "            url=self.url,\n",
    "            port=self.port,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8d5a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileProcessorMarkdownChunks:\n",
    "    INPUT_GLOB = \"../data/*\"\n",
    "    CONFIG_PATH = \"../configs/data_config.json\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.files = glob(self.INPUT_GLOB)\n",
    "        self.config = self._load_config()\n",
    "        self.splitter = self._markdown_splitter()\n",
    "        self.pdf_converter = self._pdf_converter()\n",
    "\n",
    "    def build_documents(self):\n",
    "        documents = []\n",
    "        for file in self.files:\n",
    "            documents.extend(self._process_file(file))\n",
    "        return documents\n",
    "\n",
    "    def _process_file(self, file):\n",
    "        text = self._to_markdown(Path(file))\n",
    "        docs = self.splitter.split_text(text)\n",
    "\n",
    "        for d in docs:\n",
    "            d.metadata.update(self.config[Path(file).name])\n",
    "\n",
    "        return docs\n",
    "\n",
    "    def _to_markdown(self, path: Path) -> str:\n",
    "        if path.suffix == \".pdf\":\n",
    "            return self.pdf_converter.convert(path).document.export_to_markdown()\n",
    "        if path.suffix == \".md\":\n",
    "            return path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "        raise ValueError(f\"Unsupported format: {path.suffix}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _markdown_splitter():\n",
    "        return MarkdownHeaderTextSplitter(\n",
    "            headers_to_split_on=[\n",
    "                (\"#\", \"Header 1\"),\n",
    "                (\"##\", \"Header 2\"),\n",
    "                (\"###\", \"Header 3\"),\n",
    "                (\"####\", \"Header 4\")\n",
    "            ],\n",
    "            strip_headers=False\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _pdf_converter():\n",
    "        return DocumentConverter(\n",
    "            format_options={\n",
    "                InputFormat.PDF: PdfFormatOption(\n",
    "                    pipeline_options=PdfPipelineOptions(\n",
    "                        do_ocr=True,\n",
    "                        ocr_options=EasyOcrOptions(lang=[\"en\"]),\n",
    "                        do_table_structure=False,\n",
    "                        accelerator_options=AcceleratorOptions(\n",
    "                            num_threads=4,\n",
    "                            device=AcceleratorDevice.CUDA,\n",
    "                        ),\n",
    "                    )\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_config():\n",
    "        with open(FileProcessorMarkdownChunks.CONFIG_PATH) as f:\n",
    "            return load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86b95c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileProcessorFixedSizeChunks:\n",
    "    INPUT_GLOB = \"../data/*\"\n",
    "    CONFIG_PATH = \"../configs/data_config.json\"\n",
    "\n",
    "    SPLITTERS = {\n",
    "        \".pdf\": (2090, 200),\n",
    "        \".md\": (1172, 0),\n",
    "    }\n",
    "\n",
    "    MULTIPLE_WHITESPACE = compile(r\"[ \\t]{2,}\")\n",
    "    IMAGES = compile(r\"!\\[.*?\\]\\(.*?\\)\", flags=DOTALL)\n",
    "    MULTIPLE_NEWLINES = compile(r\"\\n{2,}\")\n",
    "    LINEBREAKS = compile(r\"^\\s*---\\s*$\", flags=MULTILINE)\n",
    "    HEADINGS = compile(r\"(?=^#{1,6}\\s+)\", flags=MULTILINE)\n",
    "\n",
    "    def __init__(self):\n",
    "        self.files = glob(self.INPUT_GLOB)\n",
    "        self.config = self._load_config()\n",
    "\n",
    "    def build_documents(self):\n",
    "        documents = []\n",
    "\n",
    "        for file in self.files:\n",
    "            path = Path(file)\n",
    "            if path.suffix not in self.SPLITTERS:\n",
    "                raise ValueError(f\"Unsupported format: {path.suffix}\")\n",
    "\n",
    "            sections = self._extract_sections(path)\n",
    "            documents.extend(self._chunk(path, sections))\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def _extract_sections(self, path: Path):\n",
    "        if path.suffix == \".pdf\":\n",
    "            pages = PdfDocument(path)\n",
    "            sections = [p.get_text(\"text\") for p in pages]\n",
    "        elif path.suffix == \".md\":\n",
    "            text = path.read_text(encoding=\"utf-8\")\n",
    "            sections = self.HEADINGS.split(text)\n",
    "        \n",
    "        return (self._clean(s) for s in sections if isinstance(s, str) and s.strip())\n",
    "\n",
    "    def _chunk(self, path: Path, sections):\n",
    "        chunk_size, overlap = self.SPLITTERS[path.suffix]\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=overlap,\n",
    "        )\n",
    "\n",
    "        metadata = self.config[path.name]\n",
    "        documents = []\n",
    "\n",
    "        for section in sections:\n",
    "            for chunk in splitter.split_text(section):\n",
    "                documents.append(Document(page_content=chunk, metadata=metadata))\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def _clean(self, text: str) -> str:\n",
    "        text = \"\".join(c for c in text if c.isprintable())\n",
    "        text = self.MULTIPLE_WHITESPACE.sub(\" \", text)\n",
    "        text = self.IMAGES.sub(\"\", text)\n",
    "        text = self.LINEBREAKS.sub(\"\", text)\n",
    "        text = self.MULTIPLE_NEWLINES.sub(\"\\n\", text)\n",
    "        return text.strip()\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_config():\n",
    "        with open(FileProcessorFixedSizeChunks.CONFIG_PATH) as f:\n",
    "            return load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "709ba1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStoreMarkdownChunks(VectorStoreBase):\n",
    "    COLLECTION = \"sanath_projects_markdown_chunks\"\n",
    "    PROCESSOR_CLS = FileProcessorMarkdownChunks\n",
    "\n",
    "class VectorStoreFixedSizeChunks(VectorStoreBase):\n",
    "    COLLECTION = \"sanath_projects_fixed_size_chunks\"\n",
    "    PROCESSOR_CLS = FileProcessorFixedSizeChunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c3cff49",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class RetrievalConfig:\n",
    "    name: str\n",
    "    search_type: str\n",
    "    k: int = 10\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    experiment: str\n",
    "    strategy: str\n",
    "    ranked_documents: List[Tuple]\n",
    "\n",
    "class RetrievalExperiment:\n",
    "    def __init__(self, name: str, vector_store, reranker):\n",
    "        self.name = name\n",
    "        self.vector_store = vector_store\n",
    "        self.reranker = reranker\n",
    "\n",
    "    def run(self, query: str, config: RetrievalConfig):\n",
    "        retriever = self.vector_store.as_retriever(\n",
    "            search_type=config.search_type,\n",
    "            search_kwargs={\"k\": config.k},\n",
    "        )\n",
    "\n",
    "        documents = retriever.invoke(query)\n",
    "\n",
    "        pairs = [(query, doc.page_content) for doc in documents]\n",
    "        scores = self.reranker.predict(pairs)\n",
    "\n",
    "        ranked = [\n",
    "            (doc, score)\n",
    "            for doc, score in zip(documents, scores)\n",
    "            if score > 0\n",
    "        ]\n",
    "\n",
    "        ranked.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return ExperimentResult(\n",
    "            experiment=self.name,\n",
    "            strategy=config.name,\n",
    "            ranked_documents=ranked,\n",
    "        )\n",
    "\n",
    "class ExperimentRunner:\n",
    "    def __init__(self, experiments: list[RetrievalExperiment], configs: list[RetrievalConfig]):\n",
    "        self.experiments = experiments\n",
    "        self.configs = configs\n",
    "\n",
    "    def run_all(self, query: str) -> list[ExperimentResult]:\n",
    "        results = []\n",
    "\n",
    "        for experiment in self.experiments:\n",
    "            for config in self.configs:\n",
    "                results.append(experiment.run(query, config))\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c235f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "Verdict = Literal[\"SUFFICIENT\", \"INSUFFICIENT\"]\n",
    "Winner = Literal[\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "class SetScore(BaseModel):\n",
    "    relevance: Annotated[int, Field(ge=0, le=5)]\n",
    "    coverage: Annotated[int, Field(ge=0, le=5)]\n",
    "    noise: Annotated[int, Field(ge=0, le=5)]\n",
    "    redundancy: Annotated[int, Field(ge=0, le=5)]\n",
    "    verdict: Verdict\n",
    "\n",
    "class JudgeOutput(BaseModel):\n",
    "    winner: Winner\n",
    "    set_A: SetScore\n",
    "    set_B: SetScore\n",
    "    set_C: SetScore\n",
    "    set_D: SetScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1affd4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe(s: str) -> str:\n",
    "    return (s or \"\").strip()\n",
    "\n",
    "def format_retrieval_block(\n",
    "    ranked_documents: Sequence[tuple[Any, float]],\n",
    ") -> str:\n",
    "    lines: list[str] = []\n",
    "    for i, (doc, _) in enumerate(ranked_documents):\n",
    "        content = _safe(getattr(doc, \"page_content\", \"\"))\n",
    "\n",
    "        lines.append(\n",
    "            f\"- doc_{i+1}:\\n\"\n",
    "            f\"{content}\\n\"\n",
    "        )\n",
    "\n",
    "    return \"\\n\".join(lines) if lines else \"(no documents)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31a2d6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_judge_prompt(\n",
    "    query: str,\n",
    "    retrieval_A: str,\n",
    "    retrieval_B: str,\n",
    "    retrieval_C: str,\n",
    "    retrieval_D: str,\n",
    ") -> str:\n",
    "    return f\"\"\"\n",
    "user_query: {_safe(query)}\n",
    "\n",
    "retrieval_A:\n",
    "{retrieval_A}\n",
    "\n",
    "retrieval_B:\n",
    "{retrieval_B}\n",
    "\n",
    "retrieval_C:\n",
    "{retrieval_C}\n",
    "\n",
    "retrieval_D:\n",
    "{retrieval_D}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c64b35bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_results_to_sets(results) -> dict[str, Any]:\n",
    "    by_key = {(r.experiment, r.strategy): r for r in results}\n",
    "\n",
    "    A = by_key[(\"markdown_chunks\", \"similarity\")]\n",
    "    B = by_key[(\"markdown_chunks\", \"mmr\")]\n",
    "    C = by_key[(\"fixed_chunks\", \"similarity\")]\n",
    "    D = by_key[(\"fixed_chunks\", \"mmr\")]\n",
    "\n",
    "    return {\"A\": A, \"B\": B, \"C\": C, \"D\": D}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01517668",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \\\n",
    "\"\"\"\n",
    "You are an impartial evaluation model acting as a judge for the retrieval component of a Retrieval-Augmented Generation (RAG) system.\n",
    "\n",
    "You will compare four retrieval results for the SAME user query. Your job is to determine which set would enable a better final answer to the query.\n",
    "\n",
    "### What you will receive (each list has doc_id and content)\n",
    "1) user_query: a natural-language query\n",
    "2) retrieval_A: a list of documents\n",
    "3) retrieval_B: a list of documents\n",
    "4) retrieval_C: a list of documents\n",
    "5) retrieval_D: a list of documents\n",
    "\n",
    "### Core evaluation dimensions (apply to each set)\n",
    "1. Relevance\n",
    "   - How directly the documents address the query intent.\n",
    "\n",
    "2. Coverage / Completeness\n",
    "   - Whether the set contains enough information to answer the query fully.\n",
    "   - Identify missing key aspects.\n",
    "\n",
    "3. Noise\n",
    "   - Penalize boilerplate, metadata, formatting artifacts, tangential content, and irrelevant sections.\n",
    "   - Prefer dense, query-focused evidence.\n",
    "\n",
    "4. Redundancy\n",
    "   - Penalize excessive duplication unless it adds complementary details.\n",
    "\n",
    "### Scoring\n",
    "Score each set on a 0–5 integer scale for each dimension:\n",
    "0 = unacceptable / none\n",
    "1 = poor\n",
    "2 = weak\n",
    "3 = adequate\n",
    "4 = strong\n",
    "5 = excellent\n",
    "\n",
    "### Decision rules\n",
    "- Pick a single winner: \"A\", \"B\", \"C\", or \"D\".\n",
    "- If all sets are too weak to answer the query well, pick the less-bad winner AND mark all as insufficient.\n",
    "- Prefer the set that is simultaneously:\n",
    "  - more relevant,\n",
    "  - more complete,\n",
    "  - less noisy,\n",
    "  - and more reliable for answering.\n",
    "\n",
    "### Output format\n",
    "Do not add a preamble.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9254440",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalJudge:\n",
    "    def __init__(self, client: OpenAI, model_name: str = \"o3-mini\"):\n",
    "        self.client = client\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def judge(self, *, query: str, results) -> JudgeOutput | None:\n",
    "        sets = map_results_to_sets(results)\n",
    "\n",
    "        retrieval_A = format_retrieval_block(\n",
    "            sets[\"A\"].ranked_documents\n",
    "        )\n",
    "        retrieval_B = format_retrieval_block(\n",
    "            sets[\"B\"].ranked_documents\n",
    "        )\n",
    "        retrieval_C = format_retrieval_block(\n",
    "            sets[\"C\"].ranked_documents\n",
    "        )\n",
    "        retrieval_D = format_retrieval_block(\n",
    "            sets[\"D\"].ranked_documents\n",
    "        )\n",
    "\n",
    "        prompt = build_judge_prompt(\n",
    "            query=query,\n",
    "            retrieval_A=retrieval_A,\n",
    "            retrieval_B=retrieval_B,\n",
    "            retrieval_C=retrieval_C,\n",
    "            retrieval_D=retrieval_D,\n",
    "        )\n",
    "\n",
    "        # Structured output: let the API enforce JSON schema\n",
    "        resp = self.client.responses.parse(\n",
    "            model=self.model_name,\n",
    "            input=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": system_prompt,\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            text_format=JudgeOutput\n",
    "        )\n",
    "\n",
    "        return resp.output_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ad45b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:24:56,655 - INFO - Use pytorch device: cuda:0\n",
      "2025-12-18 20:24:57,782 - INFO - HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:24:57,801 - INFO - HTTP Request: GET http://localhost:6333/collections/sanath_projects_markdown_chunks/exists \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:24:57,862 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-18 20:25:01,633 - INFO - Going to convert document batch...\n",
      "2025-12-18 20:25:01,637 - INFO - Initializing pipeline for StandardPdfPipeline with options hash 29d7ac5255fd1ef2d81020aeb11ae19b\n",
      "2025-12-18 20:25:01,662 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-18 20:25:01,668 - INFO - Registered picture descriptions: ['vlm', 'api']\n",
      "2025-12-18 20:25:01,687 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-18 20:25:01,692 - INFO - Registered ocr engines: ['auto', 'easyocr', 'ocrmac', 'rapidocr', 'tesserocr', 'tesseract']\n",
      "2025-12-18 20:25:02,367 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-12-18 20:25:05,222 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-18 20:25:05,232 - INFO - Registered layout engines: ['docling_layout_default', 'docling_experimental_table_crops_layout']\n",
      "2025-12-18 20:25:05,253 - INFO - Accelerator device: 'cuda:0'\n",
      "2025-12-18 20:25:06,587 - INFO - Loading plugin 'docling_defaults'\n",
      "2025-12-18 20:25:06,591 - INFO - Registered table structure engines: ['docling_tableformer']\n",
      "2025-12-18 20:25:06,591 - INFO - Processing document BikeShare Data Streaming Report.pdf\n",
      "2025-12-18 20:25:38,286 - INFO - Finished converting document BikeShare Data Streaming Report.pdf in 40.45 sec.\n",
      "2025-12-18 20:25:38,542 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-18 20:25:38,557 - INFO - Going to convert document batch...\n",
      "2025-12-18 20:25:38,559 - INFO - Processing document Thesis - Layout Aware Metadata Extraction Framework Full Report.pdf\n",
      "2025-12-18 20:26:30,738 - INFO - Finished converting document Thesis - Layout Aware Metadata Extraction Framework Full Report.pdf in 52.22 sec.\n",
      "2025-12-18 20:26:31,290 - INFO - detected formats: [<InputFormat.PDF: 'pdf'>]\n",
      "2025-12-18 20:26:31,305 - INFO - Going to convert document batch...\n",
      "2025-12-18 20:26:31,305 - INFO - Processing document Working Student Report.pdf\n",
      "2025-12-18 20:26:36,800 - INFO - Finished converting document Working Student Report.pdf in 5.55 sec.\n",
      "2025-12-18 20:26:37,466 - INFO - HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:26:37,488 - INFO - HTTP Request: GET http://localhost:6333/collections/sanath_projects_markdown_chunks/exists \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:26:42,621 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:26:43,711 - INFO - HTTP Request: PUT http://localhost:6333/collections/sanath_projects_markdown_chunks \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:26:46,522 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:26:47,304 - INFO - HTTP Request: PUT http://localhost:6333/collections/sanath_projects_markdown_chunks/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:26:47,940 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:26:48,563 - INFO - HTTP Request: PUT http://localhost:6333/collections/sanath_projects_markdown_chunks/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:26:50,801 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:26:51,241 - INFO - HTTP Request: PUT http://localhost:6333/collections/sanath_projects_markdown_chunks/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:26:53,751 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:26:54,258 - INFO - HTTP Request: PUT http://localhost:6333/collections/sanath_projects_markdown_chunks/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:26:55,320 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:26:55,974 - INFO - HTTP Request: PUT http://localhost:6333/collections/sanath_projects_markdown_chunks/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:26:56,020 - INFO - HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:26:56,059 - INFO - HTTP Request: GET http://localhost:6333/collections/sanath_projects_fixed_size_chunks/exists \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:26:56,563 - INFO - HTTP Request: GET http://localhost:6333 \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:26:56,606 - INFO - HTTP Request: GET http://localhost:6333/collections/sanath_projects_fixed_size_chunks/exists \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:26:56,950 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:26:57,830 - INFO - HTTP Request: PUT http://localhost:6333/collections/sanath_projects_fixed_size_chunks \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:26:59,441 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:26:59,936 - INFO - HTTP Request: PUT http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:00,431 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:00,921 - INFO - HTTP Request: PUT http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:02,187 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:02,881 - INFO - HTTP Request: PUT http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:03,630 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:04,248 - INFO - HTTP Request: PUT http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:05,258 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:05,936 - INFO - HTTP Request: PUT http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points?wait=true \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:07,141 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:07,698 - INFO - HTTP Request: PUT http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points?wait=true \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "reranker_model = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "vs_markdown = VectorStoreMarkdownChunks().get_vector_store()\n",
    "vs_fixed = VectorStoreFixedSizeChunks().get_vector_store()\n",
    "\n",
    "experiments = [\n",
    "    RetrievalExperiment(\"markdown_chunks\", vs_markdown, reranker_model),\n",
    "    RetrievalExperiment(\"fixed_chunks\", vs_fixed, reranker_model),\n",
    "]\n",
    "\n",
    "configs = [\n",
    "    RetrievalConfig(name=\"similarity\", search_type=\"similarity\", k=10),\n",
    "    RetrievalConfig(name=\"mmr\", search_type=\"mmr\", k=10),\n",
    "]\n",
    "\n",
    "runner = ExperimentRunner(experiments, configs)\n",
    "\n",
    "client = OpenAI()\n",
    "judge = RetrievalJudge(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1da55217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:27:08,217 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:08,284 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_markdown_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e4f8c1760eb47f998fdf83009e50e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:27:08,697 - INFO - HTTP Request: GET http://localhost:6333/collections/sanath_projects_markdown_chunks \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:09,136 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:09,552 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:09,610 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_markdown_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4542056a7d254de2bc27b8b057968d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:27:10,882 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:10,920 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f0ffdb0da64f70879d825008ed4b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:27:11,023 - INFO - HTTP Request: GET http://localhost:6333/collections/sanath_projects_fixed_size_chunks \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:11,393 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:13,646 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:13,712 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d5535d515b42fda8d56f7ae5bcd65d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:27:27,774 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:28,387 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:28,425 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_markdown_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9651955401d47d48ed2b3b335653efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:27:28,653 - INFO - HTTP Request: GET http://localhost:6333/collections/sanath_projects_markdown_chunks \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:29,005 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:29,411 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:29,458 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_markdown_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b21fc20e934ccb8186c1d6393829b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:27:29,878 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:29,913 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348c556c6dcb43f8a8af5dc85b43f11c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:27:29,989 - INFO - HTTP Request: GET http://localhost:6333/collections/sanath_projects_fixed_size_chunks \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:31,061 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:31,709 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:31,777 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e4a49aa96ff4410b9ca53abf5f7e282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:27:50,673 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:51,161 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:51,209 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_markdown_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37615c5960964e6c9f026b82d5f1d6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:27:51,465 - INFO - HTTP Request: GET http://localhost:6333/collections/sanath_projects_markdown_chunks \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:54,750 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:55,115 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:55,182 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_markdown_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2888d79307c046928084ed4188dcb703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:27:57,880 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:57,918 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b2d1c718f1649daa14ab61b430beb0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:27:58,036 - INFO - HTTP Request: GET http://localhost:6333/collections/sanath_projects_fixed_size_chunks \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:58,283 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:58,701 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:27:58,745 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483a720c422b4c299d0dd3ee4bcaa9a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:28:23,686 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:28:24,500 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:28:24,543 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_markdown_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62318e0175df496d8a68b72599ddec67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:28:24,731 - INFO - HTTP Request: GET http://localhost:6333/collections/sanath_projects_markdown_chunks \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:28:25,220 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:28:25,502 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:28:25,581 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_markdown_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775e05d8df8e498d84d16d119f4da50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:28:26,557 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:28:26,584 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b764993cbcf4c2da407aa30614e9461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:28:26,621 - INFO - HTTP Request: GET http://localhost:6333/collections/sanath_projects_fixed_size_chunks \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:28:26,899 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:28:27,121 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:28:27,165 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5229c695354fc6aff2b2a2b46d852c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:28:51,131 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:28:51,634 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:28:51,680 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_markdown_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c2935a504640a99cb6c0abe3bdb750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:28:52,047 - INFO - HTTP Request: GET http://localhost:6333/collections/sanath_projects_markdown_chunks \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:28:52,795 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:28:53,177 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:28:53,210 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_markdown_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037ad1d2ee7f4573b40d9a964f7adab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:28:53,795 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:28:53,824 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab833b6b09e4c0ab36b8b712c800d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:28:53,918 - INFO - HTTP Request: GET http://localhost:6333/collections/sanath_projects_fixed_size_chunks \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:28:54,226 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:28:55,978 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:28:56,030 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c821245956b34a7ca9bac29aa281f75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:29:14,990 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:29:15,812 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:29:15,851 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_markdown_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5114c2d17b04ccfacb679118c97c9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:29:16,243 - INFO - HTTP Request: GET http://localhost:6333/collections/sanath_projects_markdown_chunks \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:29:16,623 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:29:17,040 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:29:17,083 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_markdown_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "160a46f206124792a4d235f21db09655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:29:17,751 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:29:17,812 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91b44e989654ae4867b895fc6d8994d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:29:17,912 - INFO - HTTP Request: GET http://localhost:6333/collections/sanath_projects_fixed_size_chunks \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:29:18,467 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:29:19,163 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:29:19,200 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42db560a684d49378992705cd7029b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:29:39,254 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:29:39,769 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:29:39,799 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_markdown_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b4d43cfd894accba99962d1c67dfce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:29:40,040 - INFO - HTTP Request: GET http://localhost:6333/collections/sanath_projects_markdown_chunks \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:29:40,591 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:29:41,098 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:29:41,253 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_markdown_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45fb134daa2745c2a96af4ba5974c162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:29:41,657 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:29:41,684 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "455ff15af56b4157a264b55ff56709e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:29:41,788 - INFO - HTTP Request: GET http://localhost:6333/collections/sanath_projects_fixed_size_chunks \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:29:42,170 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:29:42,533 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:29:42,593 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bbaff1fe744c36be9d7218f8fa8365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:30:02,016 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:02,602 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:02,634 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_markdown_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c559733408547308ee713cb132dcf0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:30:03,009 - INFO - HTTP Request: GET http://localhost:6333/collections/sanath_projects_markdown_chunks \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:03,528 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:03,861 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:03,897 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_markdown_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7320edd5894724ade94d15ddceb6b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:30:04,922 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:04,957 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076f3e21f6d54db49ceacad3f408deb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:30:05,114 - INFO - HTTP Request: GET http://localhost:6333/collections/sanath_projects_fixed_size_chunks \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:05,422 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:06,700 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:06,740 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb1ce4b2cd9408cbce9df4ecd6e5a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:30:19,241 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:19,851 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:19,925 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_markdown_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76bcfa8a81fb473288b57057ee4060a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:30:20,212 - INFO - HTTP Request: GET http://localhost:6333/collections/sanath_projects_markdown_chunks \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:20,770 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:22,001 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:22,036 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_markdown_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfbc43e399ef49a19f541b7ad0d265aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:30:22,719 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:22,780 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eddc56c6802b47d5adeb3ccfa56bd136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:30:22,886 - INFO - HTTP Request: GET http://localhost:6333/collections/sanath_projects_fixed_size_chunks \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:23,250 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:23,558 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:23,624 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272706d687014215889a5a869dcbe3f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:30:51,651 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:52,271 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:52,342 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_markdown_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78e62e648d04da798aac252dbbc72e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:30:52,478 - INFO - HTTP Request: GET http://localhost:6333/collections/sanath_projects_markdown_chunks \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:54,423 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:55,946 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:55,974 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_markdown_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc8daa274b44c8b90e0a0b81eaf0ca6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:30:56,837 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:56,866 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100bb745b0a444f8abd2bd2197d8a693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:30:57,004 - INFO - HTTP Request: GET http://localhost:6333/collections/sanath_projects_fixed_size_chunks \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:57,303 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:57,552 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-12-18 20:30:57,623 - INFO - HTTP Request: POST http://localhost:6333/collections/sanath_projects_fixed_size_chunks/points/query \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf079190bc9d44cbbac0756072819aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-18 20:31:13,010 - INFO - HTTP Request: POST https://api.openai.com/v1/responses \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"thesis contribution\", \n",
    "    \"llm content evaluation\", \n",
    "    \"bikes data sources\", \n",
    "    \"visualization tools used\", \n",
    "    \"table extraction configuration parameters\", \n",
    "    \"Exploratory Data Analysis for cruise ship analysis\", \n",
    "    \"Employment reference Generation\", \n",
    "    \"ETL architecture\",\n",
    "    \"chatbot LLMs used\",\n",
    "    \"what are pseudonymization and anonymization techniques?\",\n",
    "    ]\n",
    "all_res = {}\n",
    "all_outs = {}\n",
    "for query in queries:\n",
    "    results = runner.run_all(query)\n",
    "    all_res[query] = results\n",
    "    out = judge.judge(query=query, results=results)\n",
    "    if out:\n",
    "        all_outs[query] = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbb6627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_all_res(all_res: dict) -> dict:\n",
    "    serialized = {}\n",
    "\n",
    "    for query, results in all_res.items():\n",
    "        serialized[query] = []\n",
    "\n",
    "        for r in results:\n",
    "            serialized[query].append({\n",
    "                \"experiment\": r.experiment,\n",
    "                \"strategy\": r.strategy,\n",
    "                \"documents\": [\n",
    "                    {\n",
    "                        \"score\": float(score),\n",
    "                        \"page_content\": doc.page_content,\n",
    "                        \"metadata\": doc.metadata,\n",
    "                    }\n",
    "                    for doc, score in r.ranked_documents\n",
    "                ],\n",
    "            })\n",
    "\n",
    "    return serialized\n",
    "\n",
    "def serialize_all_outs(all_outs: dict) -> dict:\n",
    "    return {\n",
    "        query: out.model_dump()\n",
    "        for query, out in all_outs.items()\n",
    "    }\n",
    "\n",
    "output_dir = Path(\"../target/evaluation_outputs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "with open(output_dir / \"retrieval_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    dump(\n",
    "        serialize_all_res(all_res),\n",
    "        f,\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "with open(output_dir / \"judge_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    dump(\n",
    "        serialize_all_outs(all_outs),\n",
    "        f,\n",
    "        ensure_ascii=False,\n",
    "        indent=2,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "354d3e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thesis contribution A\n",
      "llm content evaluation A\n",
      "bikes data sources A\n",
      "visualization tools used A\n",
      "table extraction configuration parameters A\n",
      "Exploratory Data Analysis for cruise ship analysis A\n",
      "Employment reference Generation A\n",
      "ETL architecture A\n",
      "chatbot LLMs used C\n",
      "what are pseudonymization and anonymization techniques? A\n"
     ]
    }
   ],
   "source": [
    "for q, o in all_outs.items():\n",
    "    print(q, o.winner)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolio-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
